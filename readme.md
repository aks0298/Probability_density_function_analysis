# ğŸ“˜ Probability Density Function Modeling from NOâ‚‚ Data

This project estimates the **Probability Density Function (PDF)** of NOâ‚‚ concentration data using two complementary strategies:

1. Deep generative learning (GAN-inspired sample modeling)  
2. Parametric non-linear density fitting  

The workflow emphasizes data transformation, distribution learning, and density visualization using both data-driven and analytical techniques.

---

# ğŸ”¹ Project A: Sample-Based PDF Learning with GAN

## ğŸ§  Workflow

Data Loading  
â†’ Feature Selection (`no2`)  
â†’ Cleaning & Scaling  
â†’ Non-Linear Feature Mapping  
â†’ Generatorâ€“Discriminator Training  
â†’ Synthetic Sample Generation  
â†’ Density Estimation & Plotting  

---

## ğŸ“‚ Dataset

- File: `data.csv`  
- Feature Used: `no2`  
- Preprocessing:
  - Missing values removed
  - Values scaled to a bounded range for stable transformation

---

## ğŸ”„ Feature Mapping

To enrich distribution structure, values are transformed using:


Where:
- `x` = normalized NOâ‚‚ values  
- `a`, `b` = tunable shape parameters  

This mapping introduces controlled non-linearity and multi-modal behavior.

---

## ğŸ¤– Generative Density Learning

A lightweight GAN-style model is trained on the transformed samples:

- Generator learns to produce realistic transformed values  
- Discriminator evaluates real vs synthetic samples  
- Training converges toward distribution matching  

Synthetic samples from the generator are then used for density estimation.

---

## ğŸ“Š Density Approximation Methods

The learned distribution is visualized using:

- Normalized histograms  
- Kernel Density Estimation (KDE)  
- Sample-based smooth PDF curves  

---

## ğŸ¯ Goals

- Learn density without assuming a fixed distribution  
- Capture complex shapes from transformed data  
- Generate synthetic samples that follow the learned distribution  
- Visualize smooth PDFs from generated data  

---

## ğŸ“ˆ Observations

- Generated samples closely follow transformed data trends  
- KDE smoothing provides stable PDF visualization  
- Model captures skew and spread introduced by mapping  

---

# ğŸ”¹ Project B: Parametric Non-Linear PDF Fitting

## ğŸ§  Workflow

Data Loading  
â†’ Feature Selection (`no2`)  
â†’ Same Non-Linear Mapping  
â†’ Histogram Density Extraction  
â†’ Parameter Optimization  
â†’ Analytical PDF Construction  
â†’ Parameter Reporting  

---

## ğŸ”„ Transformation Consistency

The identical mapping is reused:


This keeps both approaches directly comparable.

---

## ğŸ“ Analytical Density Model

We fit a Gaussian-like exponential quadratic form:


Parameters learned from data:

- Î¼ â€” center location  
- Î» â€” spread/precision factor  
- c â€” scaling constant  

Parameters are estimated via curve fitting on empirical density.

---

## ğŸ¯ Goals

- Fit a closed-form density model  
- Estimate optimal parameters from transformed data  
- Construct an interpretable PDF equation  
- Output learned parameter values  

---

## ğŸ“ˆ Observations

- Parametric curve provides smooth global approximation  
- Strong fit near central mass of distribution  
- Less flexible in highly irregular regions (expected behavior)

---



# âœ… Result
![Parameters](results1.png)
![Histogram](results2.png)
# âœ… Summary

This work presents two PDF estimation paths from environmental NOâ‚‚ data:

- **Sample-driven generative modeling** for flexible density learning  
- **Parametric non-linear fitting** for interpretable analytical density  

Together, they illustrate the tradeoff between flexibility and interpretability in probability density estimation.
